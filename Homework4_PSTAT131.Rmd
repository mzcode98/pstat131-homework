---
title: "PSTAT 131 Homework 4"
author: "Matthew Zhang"
date: "Spring 2022"
output:
  pdf_document: default
  html_document: default
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

```{r, echo=TRUE}
library(tidyverse)
library(tidymodels)
library(corrr)
library(discrim)
library(ggplot2)
library(poissonreg)
library(klaR)

#Load data
tidymodels_prefer()
titanic <- read.csv("data/titanic.csv")

titanic$survived <-factor(titanic$survived, levels=c("Yes", "No"))
titanic$pclass <-factor(titanic$pclass)
```

## Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r, echo=TRUE}
set.seed(123)

titanic_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)

titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

titanic_split
dim(titanic_train)
dim(titanic_test)
```

```{r, echo=TRUE}
#Create recipe
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + 
                           fare, data = titanic_train) %>%
                          step_impute_linear(age) %>%
                          step_dummy(all_nominal_predictors()) %>%
                          step_interact(terms = ~ starts_with("sex"):fare) %>%
                          step_interact(terms = ~ age:fare)

titanic_recipe
```


### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r, echo=TRUE}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

In Question 2, we are performing k-folds cross validation which is a form of cross validation that takes multiple subsets of the training data to fit the model on. This is effective because it allows all observations to be input into the model which reduces bias. In essence, there are multiple iterations of validation where taking a certain fold assesses the model while the remaining are used to fit the model and thus, re-sampling.

### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

```{r, echo=TRUE}
#Logistic
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wf <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

#LDA
lda_model <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
lda_wf <- workflow() %>% 
  add_model(lda_model) %>% 
  add_recipe(titanic_recipe)

#QDA
qda_model <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
qda_wf <- workflow() %>% 
  add_model(qda_model) %>% 
  add_recipe(titanic_recipe)
```

Since there are 3 primary models, I will be fitting 30 models in total as a result of k=10 for k-folds cross validation.

### Question 5

Fit each of the models created in Question 4 to the folded data.

**IMPORTANT:** *Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of [loading and saving](https://www.r-bloggers.com/2017/04/load-save-and-rda-files/). You should still include the code to run them when you knit, but set `eval = FALSE` in the code chunks.*

```{r, echo=TRUE}
#Logistic
log_fit <- 
  log_wf %>% 
  fit_resamples(titanic_folds)

#LDA
lda_fit <- 
  lda_wf %>% 
  fit_resamples(titanic_folds)

#QDA
qda_fit <- 
  qda_wf %>% 
  fit_resamples(titanic_folds)
```

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

```{r}
#Logistic
collect_metrics(log_fit)

#LDA
collect_metrics(lda_fit)

#QDA
collect_metrics(qda_fit)
```
The best model is the Logistic model because it has the highest mean accuracy and a similar standard error to the LDA model which means it is overall evaluated to have a better performance. Additionally, the QDA model has a lower mean accuracy and higher standard error which eliminates it as a high performing model compared to the other two. 

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r, echo=TRUE}
log_fit <- fit(log_wf, titanic_train)
log_fit %>% tidy()
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

```{r, echo=TRUE}
predict(log_fit, new_data = titanic_test) %>% bind_cols(titanic_test %>% dplyr::select(survived)) %>%
  
accuracy(survived, estimate = .pred_class)
```

The model's testing accuracy is approximately the same compared to the average accuracy across folds where the average is around .806 versus .810. We can conclude that the model predicts whether or not a passenger survived from the titanic very well and that there may be a low variance.
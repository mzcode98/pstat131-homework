---
title: "PSTAT 131 Homework 6"
author: "Matthew Zhang"
date: "Spring 2022"
output:
  pdf_document: default
  word_document: default
  html_document: default
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```
```{r, echo=TRUE}
library(tidyverse)
library(tidymodels)
library(dplyr)
library(corrr)
library(discrim)
library(glmnet)
library(ggplot2)
library(janitor)
library(rpart.plot)
library(randomForest)
library(ranger)
library(vip)
library(xgboost)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r, echo=TRUE}
pokemon <- read.csv('data/pokemon.csv')
cn_pokemon <- pokemon %>% clean_names()
head(cn_pokemon)

pokemon_types <- cn_pokemon %>%
   filter(type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" | type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")

pokemon_factor <- pokemon_types %>%
  mutate(type_1 = factor(type_1)) %>%
  mutate(legendary = factor(legendary)) %>%
  mutate(generation = factor(generation))

#Initial Split
set.seed(123)

pokemon_split <- initial_split(pokemon_factor, strata = type_1, prop = 0.7)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
#Double check number of observations
dim(pokemon_train)
dim(pokemon_test)

#V-fold Cross Validation
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = "type_1")
pokemon_folds

#Create a recipe
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data= pokemon_train) %>%
                    step_dummy(legendary) %>%
                    step_dummy(generation) %>%
                    step_center(all_predictors()) %>%
                    step_scale(all_predictors())
pokemon_recipe
```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

```{r, echo=TRUE}
library(corrplot)
cor(select(pokemon_train, where(is.numeric), -c(x))) %>%
corrplot()
```

In my correlation matrix, I chose to include numerical variables and ignored the variable x because it is simply representative of the ID of the Pokemon which has no direct effect on our model. Further, we can conclude that all the variables have a positive relationship with each other with the strongest relationship between defense and sp_def. We can also see that it is slightly skewed because total is obviously a sum of all the stats and naturally will be more correlated to each of those predictors. These relationships all make sense because a majority of Pokemon need to be less powerful and have a decent balance between their respective stats.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r, echo=TRUE}
tree_spec <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification") %>% 
  set_args(cost_complexity = tune())

tree_wf <- workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_wf,
  resamples = pokemon_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```
According to the graph, it seems that a single decision tree performs better with a smaller complexity penalty and when the complexity penalty increases, the roc_auc performance drops dramatically past 0.01.

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r, echo=TRUE}
tree_roc_auc <- collect_metrics(tune_res) %>% 
  arrange(desc(mean))
tree_roc_auc
```
The best performing pruned decision tree had an roc_auc of 0.6904793 on the folds.

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r, echo=TRUE}
best <- select_best(tune_res)
tree_final <- finalize_workflow(tree_wf, best)

tree_final_fit <- fit(tree_final, data = pokemon_train)

tree_final_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()
```

### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r, echo=TRUE}
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
        set_engine("ranger", importance="impurity") %>%
    set_mode("classification")

rf_wf <- workflow() %>%
  add_recipe(pokemon_recipe) %>%
    add_model(rf_spec) 
    
rgrid <- grid_regular(mtry(range = c(2, 7)), trees(range = c(10, 1000)), min_n(range = c(2, 10)), levels = 8)
```
'mtry' denotes the number of predictors that are randomly sampled in each split iteration. 'trees' denotes the nmumber of trees in the ensemble methods. 'min_n' is representative of the minimum number of data points required in each node for a split.
It cannot be greater than 8 because there are only 8 predictors and if it were equal to 8, we would have a representative bagging model.

### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r, echo=TRUE}
rf_res <- tune_grid(
  rf_wf, 
  resamples = pokemon_folds, 
  grid = rgrid, 
  metrics = metric_set(roc_auc)
)

autoplot(rf_res)
```
Looking at the graphs, it seems that the number of trees past 200 does not seem to impact performance by that much. For predictors and node size, it seems the middle node-size has better performance.

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r, echo=TRUE}
rf_roc_auc <- collect_metrics(rf_res) %>% 
  arrange(desc(mean))
rf_roc_auc
```
The roc_auc of the best performing random forest model on the folds is 0.7360437.

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r, echo=TRUE}
best_rf <- select_best(rf_res, metric = "roc_auc")
rf_final <- finalize_workflow(rf_wf, best_rf)
rf_final_fit <- fit(rf_final, data = pokemon_train)

rf_final_fit %>% 
  extract_fit_engine() %>% 
  vip()

rf_final_fit %>% 
  extract_fit_engine() %>% 
  vip::vi() %>% 
  arrange(Importance)
```
We can see that the most useful predictor was 'sp_atk' and the least useful is which generation the Pokemon was in such as 'generation_X6'. This is not surprising since sp_atk indicates the powertype of the Pokemon which is indicative of its type. Also, generation seems to be very vague in determining a Pokemon's type.

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

```{r, echo=TRUE}
boost_spec <- boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

bt_wf <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(boost_spec)

t_grid <- grid_regular(trees(range = c(10,1000)), levels = 10)
bt_res <- tune_grid(bt_wf, resamples = pokemon_folds, grid = t_grid, metrics = metric_set(roc_auc))

autoplot(bt_res)

```

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r, echo=TRUE}
bt_roc_auc <- collect_metrics(bt_res) %>% 
  arrange(desc(mean))
bt_roc_auc
```
Looking at the graph, we can see that past 125 trees, as the number of trees increase, the roc_auc will decrease and level out.
Further, the best performing boosted tree model on the folds is 0.6931326.

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

```{r, echo=TRUE}
boosted <- (collect_metrics(bt_res) %>% arrange(desc(mean)))[1,c('.metric','mean')]
rf <- (collect_metrics(rf_res) %>% arrange(desc(mean)))[1,c('.metric','mean')]
decision <- (collect_metrics(tune_res) %>% arrange(desc(mean)))[1,c('.metric','mean')]
table <- rbind(decision, rf, boosted)
table %>% add_column(model = c("Boosted Tree", "Random Forest", "Decision Tree"), .before = ".metric")

best_rf <- select_best(rf_res, metric='roc_auc')
rf_final <- finalize_workflow(rf_wf, best_rf)
rf_final_fit <- fit(rf_final, data=pokemon_train)
```
```{r, echo=TRUE}
#AUC
roc_auc(augment(rf_final_fit, new_data = pokemon_test), type_1, .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water)

#Plot
augment(rf_final_fit, new_data = pokemon_test) %>% 
  roc_curve(type_1, .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water) %>%
  autoplot()

#Confusion
augment(rf_final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class)  %>% autoplot(type = "heatmap")
```
The model was the best at predicting the Normal types of Pokemon and the worst at predicting Fire type Pokemon.